{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><font color=\"#113D68\" size=6>Deep Learning con Python y Keras</font></h1>\n",
    "\n",
    "<h1><font color=\"#113D68\" size=5>Parte 6. Redes Neuronales Recurrentes</font></h1>\n",
    "\n",
    "<h1><font color=\"#113D68\" size=4>6. Práctica: Procesamiento del Lenguaje Natural</font></h1>\n",
    "\n",
    "<br><br>\n",
    "<div style=\"text-align: right\">\n",
    "<font color=\"#113D68\" size=3>Manuel Castillo Cara</font><br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"indice\"></a>\n",
    "<h2><font color=\"#004D7F\" size=5>Índice</font></h2>\n",
    "\n",
    "* [0. Contexto](#section0)\n",
    "* [1. Descripción del problema: generación de texto](#section1)\n",
    "* [2. LSTM de linea base](#section2)\n",
    "    * [2.1. Cargar el dataset](#section2.1)\n",
    "    * [2.2. Conversión a numérico](#section2.2)\n",
    "    * [2.3. Dimensiones del dataset](#section2.3)\n",
    "    * [2.4. Procesamiento de datos](#section2.4)\n",
    "    * [2.5. Diseño de la LSTM](#section2.5)\n",
    "    * [2.6. Crear puntos de control](#section2.6)\n",
    "    * [2.7. Resultados](#section2.7)\n",
    "* [3. Generación de texto con LSTM](#section3)\n",
    "    * [3.1. Cargar los pesos de LSTM](#section3.1)\n",
    "    * [3.2. Convertir de entero a carácter](#section3.2)\n",
    "    * [3.3. Resultados y evaluación](#section3.3)\n",
    "* [4. LSTM más profunda](#section4)\n",
    "* [5. Mejorar nuestro modelo](#section5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id=\"section0\"></a>\n",
    "# <font color=\"#004D7F\" size=6> 0. Contexto</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las redes neuronales recurrentes también se pueden utilizar como modelos generativos. Esto significa que, además de usarse para modelos predictivos (hacer predicciones), pueden aprender las secuencias de un problema y luego generar secuencias plausibles completamente nuevas para el dominio del problema. Los modelos generativos como este son útiles no solo para estudiar qué tan bien un modelo ha aprendido un problema, sino para aprender más sobre el dominio del problema en sí. \n",
    "\n",
    "En este proyecto, descubrirás cómo crear un modelo generativo de texto, carácter por carácter, utilizando redes neuronales recurrentes LSTM. Después de completar este proyecto, sabrá:\n",
    "* Dónde descargar un corpus de texto gratuito que puede utilizar para entrenar modelos generativos de texto.\n",
    "* Cómo enmarcar el problema de las secuencias de texto a un modelo generativo de redes neuronales recurrentes.\n",
    "* Cómo desarrollar un LSTM para generar secuencias de texto plausibles para un problema dado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# Eliminar warning\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section1\"></a>\n",
    "# <font color=\"#004D7F\" size=6>1. Descripción del problema: generación de texto</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muchos de los textos clásicos ya no están protegidos por derechos de autor. Esto significa que puede descargar todo el texto de estos libros de forma gratuita y utilizarlos en experimentos, como la creación de modelos generativos. Quizás el mejor lugar para acceder a libros gratuitos que ya no están protegidos por derechos de autor es el Proyecto Gutenberg. Para este proyecto vamos a utilizar el libro \"Alicia en el país de las maravillas\" de Lewis Carroll.\n",
    "\n",
    "Vamos a aprender las dependencias entre personajes y las probabilidades condicionales de personajes en secuencias para que a su vez podamos generar secuencias de personajes totalmente nuevas y originales. Este tutorial es muy divertido y recomiendo repetir estos experimentos con otros libros del Proyecto Gutenberg. Estos experimentos no se limitan al texto, también puede experimentar con otros datos ASCII, como código fuente de computadora, documentos marcados en LaTeX, HTML o Markdown y más.\n",
    "\n",
    "Puede descargar el texto completo en formato ASCII (Texto sin formato UTF-8) de este libro de forma gratuita y colocarlo en su directorio de trabajo con el nombre `wonderland.txt`. \n",
    "\n",
    "El Proyecto Gutenberg agrega un encabezado y un pie de página estándar a cada libro y esto no es parte del texto original. Abra el archivo en un editor de texto y elimine el encabezado y el pie de página. El encabezado es obvio y termina con el texto:\n",
    "```\n",
    "        *** START OF THIS PROJECT GUTENBERG EBOOK ALICE'S ADVENTURES IN WONDERLAND ***\n",
    "```\n",
    "\n",
    "El pie de página es todo el texto después de la línea de texto que dice:\n",
    "```\n",
    "        THE END\n",
    "```\n",
    "\n",
    "\n",
    "Debería quedarse con un archivo de texto que tiene aproximadamente 3330 líneas de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Más información sobre el [Proyecto Gutenberg](https://www.gutenberg.org/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    \n",
    "<i class=\"fa fa-info-circle\" aria-hidden=\"true\"></i>\n",
    "Descargar el libro [Alice's Adventures in Wonderland](https://www.gutenberg.org/ebooks/11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2\"></a>\n",
    "# <font color=\"#004D7F\" size=6>2. LSTM de linea base</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección desarrollaremos una red LSTM simple para aprender secuencias de caracteres de Alicia en el país de las maravillas. En la siguiente sección usaremos este modelo para generar nuevas secuencias de caracteres. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2.1\"></a>\n",
    "# <font color=\"#004D7F\" size=5>2.1. Cargar el dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comencemos importando las clases y funciones que pretendemos usar para entrenar nuestro modelo.\n",
    "\n",
    "A continuación, debemos cargar el texto ASCII del libro en la memoria y convertir todos los caracteres a minúsculas para reducir el vocabulario que la red debe aprender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small LSTM Network to Generate Text for Alice in Wonderland\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "# load ascii text and covert to lowercase\n",
    "filename = \"data/wonderland.txt\"\n",
    "raw_text = open(filename, 'r', encoding='utf-8').read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2.2\"></a>\n",
    "# <font color=\"#004D7F\" size=5>2.2. Conversión a numérico</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que el libro está cargado, debemos preparar los datos para modelarlos mediante la red neuronal. No podemos modelar los caracteres directamente, sino que debemos convertir los caracteres a números enteros. Podemos hacer esto fácilmente creando primero un conjunto de todos los caracteres distintos en el libro, luego creando un mapa de cada personaje a un número entero único.\n",
    "\n",
    "Por ejemplo, la lista de caracteres en minúscula ordenados únicos en el libro es la siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " ' ': 1,\n",
       " '!': 2,\n",
       " '\"': 3,\n",
       " '#': 4,\n",
       " '$': 5,\n",
       " '%': 6,\n",
       " \"'\": 7,\n",
       " '(': 8,\n",
       " ')': 9,\n",
       " '*': 10,\n",
       " ',': 11,\n",
       " '-': 12,\n",
       " '.': 13,\n",
       " '/': 14,\n",
       " '0': 15,\n",
       " '1': 16,\n",
       " '2': 17,\n",
       " '3': 18,\n",
       " '4': 19,\n",
       " '5': 20,\n",
       " '6': 21,\n",
       " '7': 22,\n",
       " '8': 23,\n",
       " '9': 24,\n",
       " ':': 25,\n",
       " ';': 26,\n",
       " '?': 27,\n",
       " '@': 28,\n",
       " '[': 29,\n",
       " ']': 30,\n",
       " '_': 31,\n",
       " 'a': 32,\n",
       " 'b': 33,\n",
       " 'c': 34,\n",
       " 'd': 35,\n",
       " 'e': 36,\n",
       " 'f': 37,\n",
       " 'g': 38,\n",
       " 'h': 39,\n",
       " 'i': 40,\n",
       " 'j': 41,\n",
       " 'k': 42,\n",
       " 'l': 43,\n",
       " 'm': 44,\n",
       " 'n': 45,\n",
       " 'o': 46,\n",
       " 'p': 47,\n",
       " 'q': 48,\n",
       " 'r': 49,\n",
       " 's': 50,\n",
       " 't': 51,\n",
       " 'u': 52,\n",
       " 'v': 53,\n",
       " 'w': 54,\n",
       " 'x': 55,\n",
       " 'y': 56,\n",
       " 'z': 57,\n",
       " 'ù': 58,\n",
       " '—': 59,\n",
       " '‘': 60,\n",
       " '’': 61,\n",
       " '“': 62,\n",
       " '”': 63,\n",
       " '\\ufeff': 64}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create mapping of unique chars to integers\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))\n",
    "char_to_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puede ver que puede haber algunos caracteres que podríamos eliminar para limpiar aún más el conjunto de datos, lo que reducirá el vocabulario y puede mejorar el proceso de modelado. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2.3\"></a>\n",
    "# <font color=\"#004D7F\" size=5>2.3. Dimensiones del dataset</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que se cargó el libro y se preparó el mapeo, podemos resumir el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters:  164202\n",
      "Total Vocab:  65\n"
     ]
    }
   ],
   "source": [
    "# summarize the loaded data\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print(\"Total Characters: \", n_chars)\n",
    "print(\"Total Vocab: \", n_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que el libro tiene poco menos de 150.000 caracteres y que cuando se convierte a minúsculas sólo hay 47 caracteres distintos en el vocabulario para que la red los aprenda. Mucho más que los 26 del alfabeto. \n",
    "\n",
    "Ahora necesitamos definir los datos de entrenamiento para la red. Hay mucha flexibilidad en la forma en que elige dividir el texto y exponerlo a la red durante el entrenamiento. En este tutorial, dividiremos el texto del libro en subsecuencias con una longitud fija de 100 caracteres, una longitud arbitraria. Podríamos dividir fácilmente los datos en oraciones y rellenar las secuencias más cortas y truncar las más largas.\n",
    "\n",
    "Cada patrón de entrenamiento de la red se compone de 100 pasos de tiempo de un carácter (X) seguidos de una salida de carácter (y). Al crear estas secuencias, deslizamos esta ventana a lo largo de todo el libro, un carácter a la vez, permitiendo que cada carácter tenga la oportunidad de aprender de los 100 caracteres que lo precedieron (excepto los primeros 100 caracteres, por supuesto). Por ejemplo, si la longitud de la secuencia es 5 (para simplificar), los dos primeros patrones de entrenamiento serían los siguientes:\n",
    "```\n",
    "            CHAPT -> E\n",
    "            HAPTE -> R\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "A medida que dividimos el libro en estas secuencias, convertimos los caracteres a números enteros usando nuestra tabla de búsqueda que preparamos anteriormente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns:  164102\n"
     ]
    }
   ],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 100\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, n_chars - seq_length, 1):\n",
    "    seq_in = raw_text[i:i + seq_length]\n",
    "    seq_out = raw_text[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "n_patterns = len(dataX)\n",
    "print(\"Total Patterns: \", n_patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejecutar el código hasta este punto nos muestra que cuando dividimos el conjunto de datos en datos de entrenamiento para que la red sepa que tenemos poco más de 150.000 patrones de entrenamiento. Esto tiene sentido ya que excluyendo los primeros 100 caracteres, tenemos un patrón de entrenamiento para predecir cada uno de los caracteres restantes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2.4\"></a>\n",
    "# <font color=\"#004D7F\" size=5>2.4. Procesamiento de datos</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que hemos preparado nuestros datos de entrenamiento, necesitamos transformarlos. \n",
    "1. Primero debemos transformar la lista de secuencias de entrada en la forma [muestras, pasos de tiempo, características] esperada por una red LSTM. \n",
    "2. A continuación, debemos cambiar la escala de los números enteros al rango de 0 a 1 para que los patrones sean más fáciles de aprender mediante la red LSTM que usa la función de activación sigmoidea de forma predeterminada.\n",
    "3. Finalmente, necesitamos convertir los patrones de salida (caracteres individuales convertidos en enteros) con One-Hot Encoding. Esto es para que podamos configurar la red para predecir la probabilidad de cada uno de los 47 caracteres diferentes en el vocabulario (una representación más fácil) en lugar de intentar forzarlo a predecir con precisión el siguiente carácter. Cada valor de _y_ se convierte en un vector disperso con una longitud de 47, lleno de ceros, excepto con un 1 en la columna de la letra (número entero) que representa el patrón. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = np.reshape(dataX, (n_patterns, seq_length, 1))\n",
    "# normalize\n",
    "X = X / float(n_vocab)\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2.5\"></a>\n",
    "# <font color=\"#004D7F\" size=5>2.5. Diseño de la LSTM</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos definir nuestro modelo LSTM. \n",
    "1. Definimos una única capa LSTM oculta con 256 unidades de memoria. \n",
    "2. La red utiliza un dropout con una probabilidad del 20%. \n",
    "3. La capa de salida es una capa densa que utiliza la función de activación Softmax para generar una predicción de probabilidad para cada uno de los 47 caracteres entre 0 y 1. \n",
    "4. El problema es realmente un problema de clasificación de un solo carácter con 47 clases y, como tal, se define como la optimización de la pérdida logarítmica (`categorical_crossentropy`)\n",
    "5. Usaremos el algoritmo de optimización de Adam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hay un conjunto de datos de prueba. Estamos modelando todo el conjunto de datos de entrenamiento para conocer la probabilidad de cada personaje en una secuencia. \n",
    "\n",
    "No estamos interesados en el modelo más preciso (Accuracy de clasificación) del conjunto de datos de entrenamiento. Este sería un modelo que predice perfectamente cada carácter en el conjunto de datos de entrenamiento. \n",
    "\n",
    "En cambio, estamos interesados en una generalización del conjunto de datos que minimice la función de pérdida elegida. Buscamos un equilibrio entre la generalización y el sobreajuste, pero sin memorizar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2.6\"></a>\n",
    "# <font color=\"#004D7F\" size=5>2.6. Crear puntos de control</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La red es lenta de entrenar. Debido a la lentitud y debido a nuestros requisitos de optimización, utilizaremos puntos de control de modelo para registrar todos los pesos de la red para archivar cada vez que se observe una mejora en la pérdida al final de la época. Usaremos el mejor conjunto de pesos (menor pérdida) para instanciar nuestro modelo generativo en la siguiente sección."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section2.7\"></a>\n",
    "# <font color=\"#004D7F\" size=5>2.7. Resultados</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora podemos ajustar nuestro modelo a los datos. Aquí utilizamos un número modesto de 20 épocas y un gran tamaño de batch de 128 patrones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1283/1283 [==============================] - ETA: 0s - loss: 3.0190\n",
      "Epoch 00001: loss improved from inf to 3.01901, saving model to weights-improvement-01-3.0190.hdf5\n",
      "1283/1283 [==============================] - 301s 235ms/step - loss: 3.0190\n",
      "Epoch 2/20\n",
      "1283/1283 [==============================] - ETA: 0s - loss: 2.8445\n",
      "Epoch 00002: loss improved from 3.01901 to 2.84446, saving model to weights-improvement-02-2.8445.hdf5\n",
      "1283/1283 [==============================] - 297s 231ms/step - loss: 2.8445\n",
      "Epoch 3/20\n",
      "1283/1283 [==============================] - ETA: 0s - loss: 2.7635\n",
      "Epoch 00003: loss improved from 2.84446 to 2.76349, saving model to weights-improvement-03-2.7635.hdf5\n",
      "1283/1283 [==============================] - 299s 233ms/step - loss: 2.7635\n",
      "Epoch 4/20\n",
      "1033/1283 [=======================>......] - ETA: 56s - loss: 2.7060"
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de ejecutar el ejemplo, debería tener varios archivos de puntos de control de peso en el directorio local. Puede eliminarlos todos excepto el que tenga el valor de pérdida más pequeño. \n",
    "\n",
    "La pérdida de red disminuyó casi en todas las épocas y espero que la red pueda beneficiarse del entrenamiento durante muchas más épocas. En la siguiente sección veremos el uso de este modelo para generar nuevas secuencias de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3\"></a>\n",
    "# <font color=\"#004D7F\" size=6>3. Generación de texto con LSTM</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La generación de texto utilizando la red LSTM entrenada es relativamente sencilla. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3.1\"></a>\n",
    "# <font color=\"#004D7F\" size=5>3.1. Cargar los pesos de LSTM</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, cargamos los datos y definimos la red exactamente de la misma manera, excepto que los pesos de la red se cargan desde un archivo de punto de control y no es necesario entrenar la red. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-19-1.9435.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3.2\"></a>\n",
    "# <font color=\"#004D7F\" size=5>3.2. Convertir de entero a carácter</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Además, al preparar el mapeo de caracteres únicos a números enteros, también debemos crear un mapeo inverso que podamos usar para convertir los números enteros de nuevo en caracteres para que podamos entender las predicciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "# summarize the loaded data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section3.3\"></a>\n",
    "# <font color=\"#004D7F\" size=5>3.3. Resultados y evaluación</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, necesitamos realmente hacer predicciones. \n",
    "\n",
    "La forma más sencilla de utilizar el modelo Keras LSTM para hacer predicciones es comenzar primero con una semilla como entrada, generar el siguiente carácter y luego actualizar la semilla para agregar el carácter generado al final y recortar el primer carácter. Este proceso se repite mientras queramos predecir nuevos caracteres (por ejemplo, una secuencia de 1000 caracteres de longitud). \n",
    "\n",
    "Podemos elegir un patrón de entrada aleatorio como nuestra secuencia semilla, luego imprimir los caracteres generados a medida que los generamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = np.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al ejecutar este ejemplo, primero se genera la semilla aleatoria seleccionada, luego cada carácter a medida que se genera. \n",
    "\n",
    "Podemos notar algunas observaciones sobre el texto generado.\n",
    "* Generalmente se ajusta al formato de línea observado en el texto original de menos de 80 caracteres antes de una nueva línea.\n",
    "* Los caracteres están separados en grupos parecidos a palabras y la mayoría de los grupos son palabras reales en inglés (por ejemplo, _the, little_ y _was),_ pero muchos no (por ejemplo, _lott, tiie_ y _taede)._\n",
    "* Algunas de las palabras en secuencia tienen sentido (por ejemplo, y el _white rabbit),_ pero muchas no (por ejemplo, _wese tilel)._\n",
    "\n",
    "El hecho de que este modelo del libro basado en caracteres produzca resultados como este es muy impresionante. Le da una idea de las capacidades de aprendizaje de las redes LSTM. Los resultados no son perfectos. \n",
    "\n",
    "En la siguiente sección, veremos cómo mejorar la calidad de los resultados mediante el desarrollo de una red LSTM mucho más grande."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section4\"></a>\n",
    "# <font color=\"#004D7F\" size=6>4. LSTM más profunda</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, podemos intentar mejorar la calidad del texto generado creando una red mucho más grande. Mantendremos el mismo número de unidades de memoria en 256, pero agregaremos una segunda capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También cambiaremos el nombre de archivo de los pesos con puntos de control para que podamos distinguir entre los pesos de esta red y la anterior (agregando la palabra más grande en el nombre del archivo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, aumentaremos el número de épocas de entrenamiento de 20 a 50 y disminuiremos el tamaño del lote de 128 a 64 para darle a la red más oportunidades de actualizarse y aprender. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the model\n",
    "model.fit(X, y, epochs=50, batch_size=64, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de ejecutar este ejemplo, puede lograr una pérdida de aproximadamente 1,2. \n",
    "\n",
    "Logrando una pérdida de 1.2219 en la época 47. Como en la sección anterior, podemos usar este mejor modelo de la ejecución para generar texto. El único cambio que debemos realizar en el script de generación de texto de la sección anterior es la especificación de la topología de la red y desde qué archivo se van a generar los pesos de la red. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"weights-improvement-47-1.2219-bigger.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "# pick a random seed\n",
    "start = np.random.randint(0, len(dataX)-1)\n",
    "pattern = dataX[start]\n",
    "print(\"Seed:\")\n",
    "print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n",
    "# generate characters\n",
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print(\"\\nDone.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que en general hay menos errores de ortografía y el texto parece más realista, pero sigue siendo bastante absurdo. Por ejemplo las mismas frases se repiten una y otra vez. \n",
    "\n",
    "Estos son mejores resultados, pero todavía hay mucho margen de mejora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section5\"></a>\n",
    "# <font color=\"#004D7F\" size=6>5. Mejorar nuestro modelo</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se muestra una muestra de ideas que tal vez desee investigar para mejorar aún más el modelo:\n",
    "* Predecir menos de 1000 caracteres como salida para una semilla determinada.\n",
    "* Eliminar toda la puntuación del texto fuente y, por tanto, del vocabulario de los modelos.\n",
    "* Pruebe un One-Hot Encoding para las secuencias de entrada.\n",
    "* Entrene al modelo en oraciones rellenas en lugar de secuencias aleatorias de caracteres.\n",
    "* Aumentar el número de épocas de entrenamiento a 100 o más.\n",
    "* Agregue Dropout a la capa de entrada visible y considere ajustar el porcentaje de Dropout.\n",
    "* Ajuste el tamaño de batch, pruebe con un tamaño de batch de 1 como línea de base (muy lenta) y tamaños más grandes a partir de ahí.\n",
    "* Agregue más unidades de memoria a las capas y / o más capas.\n",
    "* Cambie las capas de LSTM para que tengan estado para mantener el estado en todos los batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> <font size=5> <a href=\"#indice\"><i class=\"fa fa-arrow-circle-up\" aria-hidden=\"true\" style=\"color:#004D7F\"></i></a></font></div>\n",
    "\n",
    "---\n",
    "\n",
    "<div style=\"text-align: right\"> <font size=6><i class=\"fa fa-coffee\" aria-hidden=\"true\" style=\"color:#004D7F\"></i> </font></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
